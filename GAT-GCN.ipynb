{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_networkx\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\__init__.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModuleType\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m import_module\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\data\\__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Data\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhetero_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HeteroData\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtemporal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TemporalData\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\data\\data.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_sparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseTensor\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_store\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     FeatureStore,\n\u001b[0;32m     24\u001b[0m     FeatureTensorType,\n\u001b[0;32m     25\u001b[0m     TensorAttr,\n\u001b[0;32m     26\u001b[0m     _field_status,\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_store\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     EDGE_LAYOUT_TO_ATTR_NAME,\n\u001b[0;32m     30\u001b[0m     EdgeAttr,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     edge_tensor_type_to_adj_type,\n\u001b[0;32m     35\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch_sparse'"
     ]
    }
   ],
   "source": [
    "import networkx \n",
    "import gzip\n",
    "import ujson as json\n",
    "import itertools\n",
    "import treelib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import torch_geometric.utils.convert\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_sparse\n",
      "  Using cached torch_sparse-0.6.15.tar.gz (2.1 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scipy in c:\\users\\91900\\appdata\\roaming\\python\\python38\\site-packages (from torch_sparse) (1.8.0)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in c:\\users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages (from scipy->torch_sparse) (1.23.4)\n",
      "Building wheels for collected packages: torch_sparse\n",
      "  Building wheel for torch_sparse (setup.py): started\n",
      "  Building wheel for torch_sparse (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for torch_sparse\n",
      "Failed to build torch_sparse\n",
      "Installing collected packages: torch_sparse\n",
      "  Running setup.py install for torch_sparse: started\n",
      "  Running setup.py install for torch_sparse: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [50 lines of output]\n",
      "  No CUDA runtime is found, using CUDA_HOME='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA\\v11.7'\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-38\n",
      "  creating build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\add.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\bandwidth.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\cat.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\coalesce.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\convert.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\diag.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\eye.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\index_select.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\masked_select.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\matmul.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\metis.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\mul.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\narrow.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\padding.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\permute.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\reduce.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\rw.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\saint.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\sample.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\select.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\spadd.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\spmm.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\spspmm.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\storage.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\tensor.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\transpose.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\utils.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\__init__.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  running egg_info\n",
      "  writing torch_sparse.egg-info\\PKG-INFO\n",
      "  writing dependency_links to torch_sparse.egg-info\\dependency_links.txt\n",
      "  writing requirements to torch_sparse.egg-info\\requires.txt\n",
      "  writing top-level names to torch_sparse.egg-info\\top_level.txt\n",
      "  reading manifest file 'torch_sparse.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*' found under directory 'test'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file 'torch_sparse.egg-info\\SOURCES.txt'\n",
      "  running build_ext\n",
      "  C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\cpp_extension.py:358: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
      "    warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "  building 'torch_sparse._convert_cpu' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for torch_sparse\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for torch_sparse did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [52 lines of output]\n",
      "  No CUDA runtime is found, using CUDA_HOME='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA\\v11.7'\n",
      "  running install\n",
      "  C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-38\n",
      "  creating build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\add.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\bandwidth.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\cat.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\coalesce.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\convert.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\diag.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\eye.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\index_select.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\masked_select.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\matmul.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\metis.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\mul.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\narrow.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\padding.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\permute.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\reduce.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\rw.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\saint.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\sample.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\select.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\spadd.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\spmm.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\spspmm.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\storage.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\tensor.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\transpose.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\utils.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\__init__.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  running egg_info\n",
      "  writing torch_sparse.egg-info\\PKG-INFO\n",
      "  writing dependency_links to torch_sparse.egg-info\\dependency_links.txt\n",
      "  writing requirements to torch_sparse.egg-info\\requires.txt\n",
      "  writing top-level names to torch_sparse.egg-info\\top_level.txt\n",
      "  reading manifest file 'torch_sparse.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*' found under directory 'test'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file 'torch_sparse.egg-info\\SOURCES.txt'\n",
      "  running build_ext\n",
      "  C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\cpp_extension.py:358: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
      "    warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "  building 'torch_sparse._convert_cpu' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "torch_sparse\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    }
   ],
   "source": [
    "pip install torch_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-1.13.0+cpu.html\n",
      "Collecting torch-scatter\n",
      "  Downloading torch_scatter-2.0.9.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting torch-sparse\n",
      "  Downloading torch_sparse-0.6.15.tar.gz (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 5.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting torch-cluster\n",
      "  Downloading torch_cluster-1.6.0.tar.gz (43 kB)\n",
      "     ---------------------------------------- 43.4/43.4 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting torch-spline-conv\n",
      "  Downloading torch_spline_conv-1.2.1.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting torch-geometric\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "  Downloading torch_geometric-2.1.0.post1.tar.gz (467 kB)\n",
      "     -------------------------------------- 467.0/467.0 kB 9.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scipy in c:\\users\\91900\\appdata\\roaming\\python\\python38\\site-packages (from torch-sparse) (1.8.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch-geometric) (1.23.4)\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [25 lines of output]\n",
      "  No CUDA runtime is found, using CUDA_HOME='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA\\v11.7'\n",
      "  C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "    warnings.warn(\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-38\n",
      "  creating build\\lib.win-amd64-cpython-38\\torch_scatter\n",
      "  copying torch_scatter\\placeholder.py -> build\\lib.win-amd64-cpython-38\\torch_scatter\n",
      "  copying torch_scatter\\scatter.py -> build\\lib.win-amd64-cpython-38\\torch_scatter\n",
      "  copying torch_scatter\\segment_coo.py -> build\\lib.win-amd64-cpython-38\\torch_scatter\n",
      "  copying torch_scatter\\segment_csr.py -> build\\lib.win-amd64-cpython-38\\torch_scatter\n",
      "  copying torch_scatter\\utils.py -> build\\lib.win-amd64-cpython-38\\torch_scatter\n",
      "  copying torch_scatter\\__init__.py -> build\\lib.win-amd64-cpython-38\\torch_scatter\n",
      "  creating build\\lib.win-amd64-cpython-38\\torch_scatter\\composite\n",
      "  copying torch_scatter\\composite\\logsumexp.py -> build\\lib.win-amd64-cpython-38\\torch_scatter\\composite\n",
      "  copying torch_scatter\\composite\\softmax.py -> build\\lib.win-amd64-cpython-38\\torch_scatter\\composite\n",
      "  copying torch_scatter\\composite\\std.py -> build\\lib.win-amd64-cpython-38\\torch_scatter\\composite\n",
      "  copying torch_scatter\\composite\\__init__.py -> build\\lib.win-amd64-cpython-38\\torch_scatter\\composite\n",
      "  running build_ext\n",
      "  C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\cpp_extension.py:358: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
      "    warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "  building 'torch_scatter._scatter_cpu' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for torch-scatter\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [50 lines of output]\n",
      "  No CUDA runtime is found, using CUDA_HOME='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA\\v11.7'\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-38\n",
      "  creating build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\add.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     -------------------------------------- 133.1/133.1 kB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch-geometric) (2.28.1)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\91900\\appdata\\roaming\\python\\python38\\site-packages (from torch-geometric) (1.0.2)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.1-cp38-cp38-win_amd64.whl (17 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->torch-geometric) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->torch-geometric) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->torch-geometric) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->torch-geometric) (3.4)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     -------------------------------------- 298.0/298.0 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm->torch-geometric) (0.4.5)\n",
      "Building wheels for collected packages: torch-scatter, torch-sparse, torch-cluster, torch-spline-conv, torch-geometric\n",
      "  Building wheel for torch-scatter (setup.py): started\n",
      "  Building wheel for torch-scatter (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for torch-scatter\n",
      "  Building wheel for torch-sparse (setup.py): started\n",
      "  Building wheel for torch-sparse (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for torch-sparse\n",
      "  Building wheel for torch-cluster (setup.py): started\n",
      "  Building wheel for torch-cluster (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for torch-cluster\n",
      "  Building wheel for torch-spline-conv (setup.py): started\n",
      "  Building wheel for torch-spline-conv (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for torch-spline-conv\n",
      "  Building wheel for torch-geometric (setup.py): started\n",
      "  Building wheel for torch-geometric (setup.py): finished with status 'done'\n",
      "  Created wheel for torch-geometric: filename=torch_geometric-2.1.0.post1-py3-none-any.whl size=689840 sha256=57864580c5b2005c944fd362c4c408c20ab555a295b509f8a6c9711cb5cf9e59\n",
      "  Stored in directory: c:\\users\\91900\\appdata\\local\\pip\\cache\\wheels\\99\\cf\\87\\0e3b43df44e007139c943e336304552d8f76e9fdd0c0e842ae\n",
      "Successfully built torch-geometric\n",
      "Failed to build torch-scatter torch-sparse torch-cluster torch-spline-conv\n",
      "Installing collected packages: tqdm, torch-spline-conv, torch-scatter, torch-cluster, threadpoolctl, MarkupSafe, joblib, torch-sparse, jinja2, torch-geometric\n",
      "  Running setup.py install for torch-spline-conv: started\n",
      "  Running setup.py install for torch-spline-conv: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  copying torch_sparse\\bandwidth.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\cat.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\coalesce.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\convert.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\diag.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\eye.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\index_select.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\masked_select.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\matmul.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\metis.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\mul.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\narrow.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\padding.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\permute.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\reduce.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\rw.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\saint.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\sample.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\select.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\spadd.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\spmm.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\spspmm.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\storage.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\tensor.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\transpose.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\utils.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  copying torch_sparse\\__init__.py -> build\\lib.win-amd64-cpython-38\\torch_sparse\n",
      "  running egg_info\n",
      "  writing torch_sparse.egg-info\\PKG-INFO\n",
      "  writing dependency_links to torch_sparse.egg-info\\dependency_links.txt\n",
      "  writing requirements to torch_sparse.egg-info\\requires.txt\n",
      "  writing top-level names to torch_sparse.egg-info\\top_level.txt\n",
      "  reading manifest file 'torch_sparse.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*' found under directory 'test'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file 'torch_sparse.egg-info\\SOURCES.txt'\n",
      "  running build_ext\n",
      "  C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\cpp_extension.py:358: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
      "    warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "  building 'torch_sparse._convert_cpu' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for torch-sparse\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [88 lines of output]\n",
      "  No CUDA runtime is found, using CUDA_HOME='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA\\v11.7'\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-38\n",
      "  creating build\\lib.win-amd64-cpython-38\\torch_cluster\n",
      "  copying torch_cluster\\fps.py -> build\\lib.win-amd64-cpython-38\\torch_cluster\n",
      "  copying torch_cluster\\graclus.py -> build\\lib.win-amd64-cpython-38\\torch_cluster\n",
      "  copying torch_cluster\\grid.py -> build\\lib.win-amd64-cpython-38\\torch_cluster\n",
      "  copying torch_cluster\\knn.py -> build\\lib.win-amd64-cpython-38\\torch_cluster\n",
      "  copying torch_cluster\\nearest.py -> build\\lib.win-amd64-cpython-38\\torch_cluster\n",
      "  copying torch_cluster\\radius.py -> build\\lib.win-amd64-cpython-38\\torch_cluster\n",
      "  copying torch_cluster\\rw.py -> build\\lib.win-amd64-cpython-38\\torch_cluster\n",
      "  copying torch_cluster\\sampler.py -> build\\lib.win-amd64-cpython-38\\torch_cluster\n",
      "  copying torch_cluster\\__init__.py -> build\\lib.win-amd64-cpython-38\\torch_cluster\n",
      "  running egg_info\n",
      "  writing torch_cluster.egg-info\\PKG-INFO\n",
      "  writing dependency_links to torch_cluster.egg-info\\dependency_links.txt\n",
      "  writing requirements to torch_cluster.egg-info\\requires.txt\n",
      "  writing top-level names to torch_cluster.egg-info\\top_level.txt\n",
      "  reading manifest file 'torch_cluster.egg-info\\SOURCES.txt'\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\91900\\AppData\\Local\\Temp\\pip-install-wkvp9pnb\\torch-cluster_1ad0c7103ffc4a4a8584a6e66860af27\\setup.py\", line 98, in <module>\n",
      "      setup(\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\__init__.py\", line 87, in setup\n",
      "      return distutils.core.setup(**attrs)\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\_distutils\\core.py\", line 185, in setup\n",
      "      return run_commands(dist)\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\_distutils\\core.py\", line 201, in run_commands\n",
      "      dist.run_commands()\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 968, in run_commands\n",
      "      self.run_command(cmd)\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\dist.py\", line 1217, in run_command\n",
      "      super().run_command(command)\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 987, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\wheel\\bdist_wheel.py\", line 299, in run\n",
      "      self.run_command('build')\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 319, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\dist.py\", line 1217, in run_command\n",
      "      super().run_command(command)\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 987, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\_distutils\\command\\build.py\", line 132, in run\n",
      "      self.run_command(cmd_name)\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 319, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\dist.py\", line 1217, in run_command\n",
      "      super().run_command(command)\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 987, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\command\\build_py.py\", line 63, in run\n",
      "      self.build_package_data()\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\command\\build_py.py\", line 159, in build_package_data\n",
      "      for target, srcfile in self._get_package_data_output_mapping():\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\command\\build_py.py\", line 151, in _get_package_data_output_mapping\n",
      "      for package, src_dir, build_dir, filenames in self.data_files:\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\command\\build_py.py\", line 72, in __getattr__\n",
      "      self.data_files = self._get_data_files()\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\command\\build_py.py\", line 84, in _get_data_files\n",
      "      self.analyze_manifest()\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\command\\build_py.py\", line 181, in analyze_manifest\n",
      "      self.run_command('egg_info')\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 319, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\dist.py\", line 1217, in run_command\n",
      "      super().run_command(command)\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 987, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\command\\egg_info.py\", line 308, in run\n",
      "      self.find_sources()\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\command\\egg_info.py\", line 316, in find_sources\n",
      "      mm.run()\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\command\\egg_info.py\", line 560, in run\n",
      "      self.add_defaults()\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\command\\egg_info.py\", line 604, in add_defaults\n",
      "      self.read_manifest()\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\command\\sdist.py\", line 209, in read_manifest\n",
      "      self.filelist.append(line)\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\command\\egg_info.py\", line 497, in append\n",
      "      path = convert_path(item)\n",
      "    File \"C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\_distutils\\util.py\", line 139, in convert_path\n",
      "      raise ValueError(\"path '%s' cannot be absolute\" % pathname)\n",
      "  ValueError: path '/Users/rusty1s/github/pytorch_cluster/csrc/fps.cpp' cannot be absolute\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for torch-cluster\n",
      "  error: subprocess-exited-with-error\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [22 lines of output]\n",
      "  No CUDA runtime is found, using CUDA_HOME='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA\\v11.7'\n",
      "  C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "    warnings.warn(\n",
      "  C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.\n",
      "    warnings.warn(\n",
      "  running bdist_wheel\n",
      "  C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "    warnings.warn(msg.format('we could not find ninja.'))\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-38\n",
      "  creating build\\lib.win-amd64-cpython-38\\torch_spline_conv\n",
      "  copying torch_spline_conv\\basis.py -> build\\lib.win-amd64-cpython-38\\torch_spline_conv\n",
      "  copying torch_spline_conv\\conv.py -> build\\lib.win-amd64-cpython-38\\torch_spline_conv\n",
      "  copying torch_spline_conv\\weighting.py -> build\\lib.win-amd64-cpython-38\\torch_spline_conv\n",
      "  copying torch_spline_conv\\__init__.py -> build\\lib.win-amd64-cpython-38\\torch_spline_conv\n",
      "  running build_ext\n",
      "  C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\cpp_extension.py:358: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
      "    warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "  building 'torch_spline_conv._basis_cpu' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for torch-spline-conv\n",
      "  WARNING: The script tqdm.exe is installed in 'C:\\Users\\91900\\anaconda3\\envs\\myenv\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for torch-spline-conv did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [24 lines of output]\n",
      "  No CUDA runtime is found, using CUDA_HOME='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA\\v11.7'\n",
      "  C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "    warnings.warn(\n",
      "  C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.\n",
      "    warnings.warn(\n",
      "  running install\n",
      "  C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-38\n",
      "  creating build\\lib.win-amd64-cpython-38\\torch_spline_conv\n",
      "  copying torch_spline_conv\\basis.py -> build\\lib.win-amd64-cpython-38\\torch_spline_conv\n",
      "  copying torch_spline_conv\\conv.py -> build\\lib.win-amd64-cpython-38\\torch_spline_conv\n",
      "  copying torch_spline_conv\\weighting.py -> build\\lib.win-amd64-cpython-38\\torch_spline_conv\n",
      "  copying torch_spline_conv\\__init__.py -> build\\lib.win-amd64-cpython-38\\torch_spline_conv\n",
      "  running build_ext\n",
      "  C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "    warnings.warn(msg.format('we could not find ninja.'))\n",
      "  C:\\Users\\91900\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\cpp_extension.py:358: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
      "    warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "  building 'torch_spline_conv._basis_cpu' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "torch-spline-conv\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.13.0+cpu.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(networkx.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tree_user_edges(tree, tweets):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - tree:\n",
    "            recursive tree structure\n",
    "            {tweet: \"tweet_id\", replies: [ .... ]}\n",
    "    Output:\n",
    "        - list of replier, poster user id pairs\n",
    "          (poster <- replier)\n",
    "    \"\"\"\n",
    "\n",
    "    parent_tweet_id = tree[\"tweet\"]\n",
    "    parent_user_id = tweets[parent_tweet_id][\"user_id\"]\n",
    "\n",
    "    edges_from_children = []\n",
    "    downstream_edges = []\n",
    "\n",
    "    for reply in tree[\"replies\"]:\n",
    "        reply_tweet_id = reply[\"tweet\"]\n",
    "        reply_user_id = tweets[reply_tweet_id][\"user_id\"]\n",
    "\n",
    "        # add an edge from the parent to the reply\n",
    "        edges_from_children.append((reply_user_id, parent_user_id))\n",
    "\n",
    "        # recursively get the edges of the child\n",
    "        downstream_edges += get_tree_user_edges(reply, tweets)\n",
    "\n",
    "    return edges_from_children + downstream_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tree_to_nx_user_graph(conversation, directed=True, remove_root=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - conversation dictionary:\n",
    "        {\n",
    "            \"tweets\": metadata for each tweet in the tree\n",
    "            \"reply_tree\": recursive tree structure\n",
    "        }\n",
    "    Output:\n",
    "        - networkx directed graph\n",
    "    \"\"\"\n",
    "\n",
    "    # extract key fields\n",
    "    tree = conversation[\"reply_tree\"]\n",
    "    tweets = conversation[\"tweets\"]\n",
    "    \n",
    "    # fetch the root user id\n",
    "    root_tweet_id = tree[\"tweet\"]\n",
    "    root_user_id = tweets[root_tweet_id][\"user_id\"]\n",
    "    \n",
    "    # nodes: unique user ids\n",
    "    nodes = {tweet[\"user_id\"] for tweet in tweets.values()}\n",
    "    \n",
    "    # edges: list of (replier, poster) pairs\n",
    "    edges = get_tree_user_edges(tree, tweets)\n",
    "\n",
    "    # remove self looops\n",
    "    edges = [(u, v) for u, v in edges if u != v]\n",
    "        \n",
    "    # remove the root user, if necessary\n",
    "    if remove_root:\n",
    "        nodes = {u_id for u_id in nodes if u_id != root_user_id}\n",
    "        edges = [(u, v) for u, v in edges \n",
    "                 if u != root_user_id and v != root_user_id]\n",
    "    \n",
    "    # create networkx graph\n",
    "    G = networkx.Graph()\n",
    "    \n",
    "    if directed:\n",
    "        G = networkx.DiGraph()\n",
    "        \n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd=r\"C:\\Users\\91900\\NAM\\final\"\n",
    "data_arr=[]\n",
    "for filename in os.listdir(cwd):\n",
    "   with open(os.path.join(cwd, filename),encoding=\"utf8\", errors='ignore') as f: # open in readonly mode\n",
    "      # do your stuff\n",
    "      conversation=json.load(f)\n",
    "      d=dict()\n",
    "      G = tree_to_nx_user_graph(conversation, directed=True, remove_root=True)\n",
    "      for tids in conversation['tweets']:\n",
    "        #print(tids)\n",
    "        d[conversation['tweets'][tids]['user_id']]={\"time\":conversation['tweets'][tids]['time'],\"tox-score\":conversation['tweets'][tids][\"tox-score\"],\"alignment-score\":conversation['tweets'][tids]['alignment-score']}\n",
    "#      networkx.draw(G_reply,with_labels=True)\n",
    "      #print(d)\n",
    "      networkx.set_node_attributes(G, d)\n",
    "      #print(networkx.get_node_attributes(G, 'time'))\n",
    "      \n",
    "      #for nodes in G.nodes():\n",
    "            #print(G[nodes])\n",
    "      g_data=from_networkx(G,group_node_attrs=[\"time\",\"tox-score\",\"alignment-score\"])\n",
    "      g_data.y=conversation['ftox']\n",
    "      #print(conversation['ftox'])\n",
    "      #print(g_data.to_dict())\n",
    "      data_arr.append(g_data)\n",
    "      \n",
    "#data=data_arr\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=data_arr\n",
    "data=dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from random import shuffle\n",
    "torch.manual_seed(12345)\n",
    "shuffle(dataset)\n",
    "\n",
    "train_dataset = dataset[:700]\n",
    "test_dataset = dataset[700:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv,GNNExplainer\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(456)\n",
    "        self.conv1 = GCNConv(3, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, 2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        #x = x.relu()\n",
    "        #x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "         print(pred)\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        self.hid = 8\n",
    "        self.in_head = 8\n",
    "        self.out_head = 1\n",
    "        \n",
    "        \n",
    "        self.conv1 = GATConv(3, self.hid, heads=self.in_head, dropout=0.4)\n",
    "        self.conv2 = GATConv(self.hid*self.in_head, 2, concat=False,\n",
    "                             heads=self.out_head, dropout=0.4)\n",
    "        #self.lin = Linear(hidden_channels, 2)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        #x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "        #return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cpu\"\n",
    "\n",
    "model = GAT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import labml_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from labml_helpers.module import Module\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(Module):\n",
    "    \"\"\"\n",
    "    ## Graph attention layer\n",
    "    This is a single graph attention layer.\n",
    "    A GAT is made up of multiple such layers.\n",
    "    It takes\n",
    "    $$\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$$,\n",
    "    where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input\n",
    "    and outputs\n",
    "    $$\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$$,\n",
    "    where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int = 8, out_features: int = 8, n_heads: int = 1,\n",
    "                 is_concat: bool = True,\n",
    "                 dropout: float = 0.6,\n",
    "                 leaky_relu_negative_slope: float = 0.2):\n",
    "        \"\"\"\n",
    "        * `in_features`, $F$, is the number of input features per node\n",
    "        * `out_features`, $F'$, is the number of output features per node\n",
    "        * `n_heads`, $K$, is the number of attention heads\n",
    "        * `is_concat` whether the multi-head results should be concatenated or averaged\n",
    "        * `dropout` is the dropout probability\n",
    "        * `leaky_relu_negative_slope` is the negative slope for leaky relu activation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Calculate the number of dimensions per head\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0\n",
    "            # If we are concatenating the multiple heads\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            # If we are averaging the multiple heads\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        # Linear layer for initial transformation;\n",
    "        # i.e. to transform the node embeddings before self-attention\n",
    "        self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)\n",
    "        # Linear layer to compute attention score $e_{ij}$\n",
    "        self.attn = nn.Linear(self.n_hidden * 2, 1, bias=False)\n",
    "        # The activation for attention score $e_{ij}$\n",
    "        self.activation = nn.LeakyReLU(negative_slope=leaky_relu_negative_slope)\n",
    "        # Softmax to compute attention $\\alpha_{ij}$\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        # Dropout layer to be applied for attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `h`, $\\mathbf{h}$ is the input node embeddings of shape `[n_nodes, in_features]`.\n",
    "        * `adj_mat` is the adjacency matrix of shape `[n_nodes, n_nodes, n_heads]`.\n",
    "        We use shape `[n_nodes, n_nodes, 1]` since the adjacency is the same for each head.\n",
    "        Adjacency matrix represent the edges (or connections) among nodes.\n",
    "        `adj_mat[i][j]` is `True` if there is an edge from node `i` to node `j`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Number of nodes\n",
    "        n_nodes = h.shape[0]\n",
    "        # The initial transformation,\n",
    "        # $$\\overrightarrow{g^k_i} = \\mathbf{W}^k \\overrightarrow{h_i}$$\n",
    "        # for each head.\n",
    "        # We do single linear transformation and then split it up for each head.\n",
    "        g = self.linear(h).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "\n",
    "        # #### Calculate attention score\n",
    "        #\n",
    "        # We calculate these for each head $k$. *We have omitted $\\cdot^k$ for simplicity*.\n",
    "        #\n",
    "        # $$e_{ij} = a(\\mathbf{W} \\overrightarrow{h_i}, \\mathbf{W} \\overrightarrow{h_j}) =\n",
    "        # a(\\overrightarrow{g_i}, \\overrightarrow{g_j})$$\n",
    "        #\n",
    "        # $e_{ij}$ is the attention score (importance) from node $j$ to node $i$.\n",
    "        # We calculate this for each head.\n",
    "        #\n",
    "        # $a$ is the attention mechanism, that calculates the attention score.\n",
    "        # The paper concatenates\n",
    "        # $\\overrightarrow{g_i}$, $\\overrightarrow{g_j}$\n",
    "        # and does a linear transformation with a weight vector $\\mathbf{a} \\in \\mathbb{R}^{2 F'}$\n",
    "        # followed by a $\\text{LeakyReLU}$.\n",
    "        #\n",
    "        # $$e_{ij} = \\text{LeakyReLU} \\Big(\n",
    "        # \\mathbf{a}^\\top \\Big[\n",
    "        # \\overrightarrow{g_i} \\Vert \\overrightarrow{g_j}\n",
    "        # \\Big] \\Big)$$\n",
    "\n",
    "        # First we calculate\n",
    "        # $\\Big[\\overrightarrow{g_i} \\Vert \\overrightarrow{g_j} \\Big]$\n",
    "        # for all pairs of $i, j$.\n",
    "        #\n",
    "        # `g_repeat` gets\n",
    "        # $$\\{\\overrightarrow{g_1}, \\overrightarrow{g_2}, \\dots, \\overrightarrow{g_N},\n",
    "        # \\overrightarrow{g_1}, \\overrightarrow{g_2}, \\dots, \\overrightarrow{g_N}, ...\\}$$\n",
    "        # where each node embedding is repeated `n_nodes` times.\n",
    "        g_repeat = g.repeat(n_nodes, 1, 1)\n",
    "        # `g_repeat_interleave` gets\n",
    "        # $$\\{\\overrightarrow{g_1}, \\overrightarrow{g_1}, \\dots, \\overrightarrow{g_1},\n",
    "        # \\overrightarrow{g_2}, \\overrightarrow{g_2}, \\dots, \\overrightarrow{g_2}, ...\\}$$\n",
    "        # where each node embedding is repeated `n_nodes` times.\n",
    "        g_repeat_interleave = g.repeat_interleave(n_nodes, dim=0)\n",
    "        # Now we concatenate to get\n",
    "        # $$\\{\\overrightarrow{g_1} \\Vert \\overrightarrow{g_1},\n",
    "        # \\overrightarrow{g_1} \\Vert \\overrightarrow{g_2},\n",
    "        # \\dots, \\overrightarrow{g_1}  \\Vert \\overrightarrow{g_N},\n",
    "        # \\overrightarrow{g_2} \\Vert \\overrightarrow{g_1},\n",
    "        # \\overrightarrow{g_2} \\Vert \\overrightarrow{g_2},\n",
    "        # \\dots, \\overrightarrow{g_2}  \\Vert \\overrightarrow{g_N}, ...\\}$$\n",
    "        g_concat = torch.cat([g_repeat_interleave, g_repeat], dim=-1)\n",
    "        # Reshape so that `g_concat[i, j]` is $\\overrightarrow{g_i} \\Vert \\overrightarrow{g_j}$\n",
    "        g_concat = g_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
    "\n",
    "        # Calculate\n",
    "        # $$e_{ij} = \\text{LeakyReLU} \\Big(\n",
    "        # \\mathbf{a}^\\top \\Big[\n",
    "        # \\overrightarrow{g_i} \\Vert \\overrightarrow{g_j}\n",
    "        # \\Big] \\Big)$$\n",
    "        # `e` is of shape `[n_nodes, n_nodes, n_heads, 1]`\n",
    "        e = self.activation(self.attn(g_concat))\n",
    "        # Remove the last dimension of size `1`\n",
    "        e = e.squeeze(-1)\n",
    "\n",
    "        # The adjacency matrix should have shape\n",
    "        # `[n_nodes, n_nodes, n_heads]` or`[n_nodes, n_nodes, 1]`\n",
    "        assert adj_mat.shape[0] == 1 or adj_mat.shape[0] == n_nodes\n",
    "        assert adj_mat.shape[1] == 1 or adj_mat.shape[1] == n_nodes\n",
    "        assert adj_mat.shape[2] == 1 or adj_mat.shape[2] == self.n_heads\n",
    "        # Mask $e_{ij}$ based on adjacency matrix.\n",
    "        # $e_{ij}$ is set to $- \\infty$ if there is no edge from $i$ to $j$.\n",
    "        e = e.masked_fill(adj_mat == 0, float('-inf'))\n",
    "\n",
    "        # We then normalize attention scores (or coefficients)\n",
    "        # $$\\alpha_{ij} = \\text{softmax}_j(e_{ij}) =\n",
    "        # \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}$$\n",
    "        #\n",
    "        # where $\\mathcal{N}_i$ is the set of nodes connected to $i$.\n",
    "        #\n",
    "        # We do this by setting unconnected $e_{ij}$ to $- \\infty$ which\n",
    "        # makes $\\exp(e_{ij}) \\sim 0$ for unconnected pairs.\n",
    "        a = self.softmax(e)\n",
    "\n",
    "        # Apply dropout regularization\n",
    "        a = self.dropout(a)\n",
    "\n",
    "        # Calculate final output for each head\n",
    "        # $$\\overrightarrow{h'^k_i} = \\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\overrightarrow{g^k_j}$$\n",
    "        #\n",
    "        # *Note:* The paper includes the final activation $\\sigma$ in $\\overrightarrow{h_i}$\n",
    "        # We have omitted this from the Graph Attention Layer implementation\n",
    "        # and use it on the GAT model to match with how other PyTorch modules are defined -\n",
    "        # activation as a separate layer.\n",
    "        attn_res = torch.einsum('ijh,jhf->ihf', a, g)\n",
    "\n",
    "        # Concatenate the heads\n",
    "        if self.is_concat:\n",
    "            # $$\\overrightarrow{h'_i} = \\Bigg\\Vert_{k=1}^{K} \\overrightarrow{h'^k_i}$$\n",
    "            return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
    "        # Take the mean of the heads\n",
    "        else:\n",
    "            # $$\\overrightarrow{h'_i} = \\frac{1}{K} \\sum_{k=1}^{K} \\overrightarrow{h'^k_i}$$\n",
    "            return attn_res.mean(dim=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cpu\"\n",
    "\n",
    "model = GraphAttentionLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "---\n",
    "title: Train a Graph Attention Network v2 (GATv2) on Cora dataset\n",
    "summary: >\n",
    "  This trains is a  Graph Attention Network v2 (GATv2) on Cora dataset\n",
    "---\n",
    "\n",
    "# Train a Graph Attention Network v2 (GATv2) on Cora dataset\n",
    "\n",
    "[![View Run](https://img.shields.io/badge/labml-experiment-brightgreen)](https://app.labml.ai/run/34b1e2f6ed6f11ebb860997901a2d1e3)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from labml import experiment\n",
    "from labml.configs import option\n",
    "from labml_helpers.module import Module\n",
    "from labml_nn.graphs.gat.experiment import Configs as GATConfigs\n",
    "from labml_nn.graphs.gatv2 import GraphAttentionV2Layer\n",
    "\n",
    "\n",
    "class GATv2(Module):\n",
    "    \"\"\"\n",
    "    ## Graph Attention Network v2 (GATv2)\n",
    "\n",
    "    This graph attention network has two [graph attention layers](index.html).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, n_hidden: int, n_classes: int, n_heads: int, dropout: float,\n",
    "                 share_weights: bool = True):\n",
    "        \"\"\"\n",
    "        * `in_features` is the number of features per node\n",
    "        * `n_hidden` is the number of features in the first graph attention layer\n",
    "        * `n_classes` is the number of classes\n",
    "        * `n_heads` is the number of heads in the graph attention layers\n",
    "        * `dropout` is the dropout probability\n",
    "        * `share_weights` if set to True, the same matrix will be applied to the source and the target node of every edge\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # First graph attention layer where we concatenate the heads\n",
    "        self.layer1 = GraphAttentionV2Layer(in_features, n_hidden, n_heads,\n",
    "                                            is_concat=True, dropout=dropout, share_weights=share_weights)\n",
    "        # Activation function after first graph attention layer\n",
    "        self.activation = nn.ELU()\n",
    "        # Final graph attention layer where we average the heads\n",
    "        self.output = GraphAttentionV2Layer(n_hidden, n_classes, 1,\n",
    "                                            is_concat=False, dropout=dropout, share_weights=share_weights)\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, adj_mat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` is the features vectors of shape `[n_nodes, in_features]`\n",
    "        * `adj_mat` is the adjacency matrix of the form\n",
    "         `[n_nodes, n_nodes, n_heads]` or `[n_nodes, n_nodes, 1]`\n",
    "        \"\"\"\n",
    "        # Apply dropout to the input\n",
    "        x = self.dropout(x)\n",
    "        # First graph attention layer\n",
    "        x = self.layer1(x, adj_mat)\n",
    "        # Activation function\n",
    "        x = self.activation(x)\n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        # Output layer (without activation) for logits\n",
    "        return self.output(x, adj_mat)\n",
    "\n",
    "\n",
    "class Configs(GATConfigs):\n",
    "    \"\"\"\n",
    "    ## Configurations\n",
    "\n",
    "    Since the experiment is same as [GAT experiment](../gat/experiment.html) but with\n",
    "    [GATv2 model](index.html) we extend the same configs and change the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Whether to share weights for source and target nodes of edges\n",
    "    share_weights: bool = False\n",
    "    # Set the model\n",
    "    model: GATv2 = 'gat_v2_model'\n",
    "\n",
    "\n",
    "@option(Configs.model)\n",
    "def gat_v2_model(c: Configs):\n",
    "    \"\"\"\n",
    "    Create GATv2 model\n",
    "    \"\"\"\n",
    "    return GATv2(c.in_features, c.n_hidden, c.n_classes, c.n_heads, c.dropout, c.share_weights).to(c.device)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Create configurations\n",
    "    conf = Configs()\n",
    "    # Create an experiment\n",
    "    experiment.create(name='gatv2')\n",
    "    # Calculate configurations.\n",
    "    experiment.configs(conf, {\n",
    "        # Adam optimizer\n",
    "        'optimizer.optimizer': 'Adam',\n",
    "        'optimizer.learning_rate': 5e-3,\n",
    "        'optimizer.weight_decay': 5e-4,\n",
    "\n",
    "        'dropout': 0.7,\n",
    "    })\n",
    "\n",
    "    # Start and watch the experiment\n",
    "    with experiment.start():\n",
    "        # Run the training\n",
    "        conf.run()\n",
    "\n",
    "\n",
    "#\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         print(pred)\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, edge_index = dataset[225].x, dataset[225].edge_index\n",
    "explainer = GNNExplainer(model, epochs=200, return_type='raw', log=False)\n",
    "node_feat_mask, edge_mask = explainer.explain_graph(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_subgraph_mutag(graph: nx.Graph,\n",
    "                             node_set: Optional[Set[int]] = None,\n",
    "                             edge_set: Optional[Set[int]] = None,\n",
    "                             title: Optional[str] = None) -> None:\n",
    "    \"\"\"Visualizes a subgraph explanation for a graph from the MUTAG dataset.\n",
    "\n",
    "    Note: Only provide subgraph_node_set or subgraph_edge_set, not both.\n",
    "\n",
    "    Adapted from https://github.com/divelab/DIG/blob/dig/dig/xgraph/method/subgraphx.py\n",
    "\n",
    "    :param graph: A NetworkX graph object representing the full graph.\n",
    "    :param node_set: A set of nodes that induces a subgraph.\n",
    "    :param edge_set: A set of edges that induces a subgraph.\n",
    "    :param title: Optional title for the plot.\n",
    "    \"\"\"\n",
    "    if node_set is None:\n",
    "        node_set = set(graph.nodes())\n",
    "\n",
    "    if edge_set is None:\n",
    "        edge_set = {(n_from, n_to) for (n_from, n_to) in graph.edges() if n_from in node_set and n_to in node_set}\n",
    "\n",
    "    node_dict = {0: 'C', 1: 'N', 2: 'O', 3: 'F', 4: 'I', 5: 'Cl', 6: 'Br'}\n",
    "    node_idxs = {node: node_x.index(1.0) for node, node_x in graph.nodes(data='x')}\n",
    "    node_labels = {k: node_dict[v] for k, v in node_idxs.items()}\n",
    "    node_color = ['#E49D1C', '#4970C6', '#FF5357', '#29A329', 'brown', 'darkslategray', '#F0EA00']\n",
    "    colors = [node_color[v % len(node_color)] for k, v in node_idxs.items()]\n",
    "\n",
    "    pos = nx.kamada_kawai_layout(graph)\n",
    "\n",
    "    nx.draw_networkx_nodes(G=graph, pos=pos, nodelist=list(graph.nodes()), node_color=colors, node_size=300)\n",
    "    nx.draw_networkx_edges(G=graph, pos=pos, width=3, edge_color='gray', arrows=False)\n",
    "    nx.draw_networkx_edges(G=graph, pos=pos, edgelist=list(edge_set), width=6, edge_color='black', arrows=False)\n",
    "    nx.draw_networkx_labels(G=graph, pos=pos, labels=node_labels)\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_explanation_for_graph(threshold: float, graph_idx: int) -> None:\n",
    "    \"\"\"Visualizes the explanations of GNNExplainer for a graph given a mask threshold.\"\"\"\n",
    "    mutag_data = test_dataset[graph_idx]\n",
    "\n",
    "    _, edge_mask = explainer.explain_graph(mutag_data.x.to(device), mutag_data.edge_index.to(device))\n",
    "\n",
    "    batch = torch.zeros(mutag_data.x.shape[0], dtype=int, device=device)\n",
    "    output = mutag_model(mutag_data.x.to(device), mutag_data.edge_index.to(device), batch)\n",
    "    pred = torch.sigmoid(output).item()\n",
    "\n",
    "    edge_set = {(edge[0].item(), edge[1].item()) for edge, mask in zip(mutag_data.edge_index.T, edge_mask) if mask > threshold}\n",
    "    graph = to_networkx(mutag_data, node_attrs=['x'], edge_attrs=['edge_attr'], to_undirected=True)\n",
    "\n",
    "    visualize_subgraph_mutag(\n",
    "        graph=graph,\n",
    "        edge_set=edge_set,\n",
    "        title=f'GNNExplainer on graph {graph_idx}: label = {mutag_data.y.item()}, pred = {pred:.2f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    visualize_explanation_for_graph(threshold=0.5, graph_idx=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(node_feat_mask)\n",
    "print(edge_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 10\n",
    "\n",
    "# Visualize result\n",
    "ax, G = explainer.visualize_subgraph(35, edge_index, edge_mask, y=data.y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
